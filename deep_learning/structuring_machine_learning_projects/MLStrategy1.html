<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Chinmay Hundekari" />
  <meta name="dcterms.date" content="2020-05-07" />
  <title>Machine learning Strategy</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Machine learning Strategy</h1>
<p class="author">Chinmay Hundekari</p>
<p class="date">May 07, 2020</p>
</header>
<nav id="TOC" role="doc-toc">
<h2 id="toc-title">Table of Contents</h2>
<ul>
<li><a href="#to-execute">To execute:</a></li>
<li><a href="#orthogonalization">Orthogonalization</a></li>
<li><a href="#single-number-evaluation-metric">Single number evaluation metric</a>
<ul>
<li><a href="#metrics">Metrics</a></li>
</ul></li>
<li><a href="#trainingdevtest-distributions">Training/dev/test distributions</a></li>
</ul>
</nav>
<h1 id="to-execute">To execute:</h1>
<pre><code>pandoc MLStrategy.md -f markdown+tex_math_dollars -s -o MLStrategy.html --mathjax --toc -V toc-title:&quot;Table of Contents&quot;</code></pre>
<h1 id="orthogonalization">Orthogonalization</h1>
<ul>
<li>1 variable to be tuned with 1 purpose, simplifies tuning.</li>
<li>Orthogonal means at <span class="math inline">\(90\deg\)</span> to each other. Each parameter is independent of others.</li>
<li>Chain of assumptions in ML</li>
</ul>
<ol type="1">
<li>Fit training set well on cost function –&gt; Bigger network, ADAM, better optimizer</li>
<li>Fit dev set well on cost functions. –&gt; Regularization, Bigger training set</li>
<li>Fit test set well on cost function. –&gt; Bigger dev set</li>
<li>Performs well in the world. –&gt; Change dev set or cost function.</li>
</ol>
<ul>
<li>Early stopping is less orthogonal, as it affects multiple features, hence not preferred.</li>
</ul>
<h1 id="single-number-evaluation-metric">Single number evaluation metric</h1>
<h2 id="metrics">Metrics</h2>
<ul>
<li><p>Positive Samples - Part of the classification.</p></li>
<li><p>Negative Samples - Not part of the classification.</p></li>
<li><p>True Positives - Correctly Identified Positive Samples</p></li>
<li><p>False Positives - Incorrectly Identified Positive Samples</p></li>
<li><p>True Negatives - Correctly Identified Negative Samples</p></li>
<li><p>False Negatives - Incorrectly Identified Negative Samples</p></li>
<li><p>Accuracy - <span class="math display">\[ Accuracy = \frac{TP + TN}{TP + TN + FP + FN} \]</span></p></li>
<li><p>Accuracy may not accurately represent a class imbalanced data set, where TP are few.</p></li>
<li><p>Precision - What percentage of recognized class were correctly recognized? or True Positives. <span class="math display">\[ Precision = \frac{TP}{TP + FP} \]</span></p></li>
<li><p>Recall - What percentage of all positive samples were recognized? or True Positives/(All Positives) $$ Recall =  $</p></li>
<li><p>Receiver Operating Characteristic Curve - Graph of TP Rate vs FP Rate <span class="math display">\[ True Postive Rate(TPR) = \frac{TP}{TP + FN}
\]</span> False Postive Rate(FPR) = </p></li>
<li><p>F1 Score - Harmonic mean of P and R <span class="math display">\[ F1 = \frac{2}{\frac{1}{P} + \frac{1}{R}} \]</span></p></li>
<li><p>Satisficing metric - certain conditions should be satisfied. - e.g. running time should be less than 100ms.</p></li>
<li><p>Optimizing metric - Certain conditions need to be as good aspossible - e.g. Accuracy.</p></li>
</ul>
<h1 id="trainingdevtest-distributions">Training/dev/test distributions</h1>
<ul>
<li>Dev and test sets should be from same distribution</li>
<li>Purpose of test set is only to provide a high confidence.</li>
<li>Deep learning algorithms require a lot of data, pushing training data to be maximum of all samples.</li>
<li>Evaluation metric should not confuse Positives and Negatives. e.g. Scenario - Spam should not be allowed - Positives are spam</li>
<li>Evaluation metric should not ignore Negatives which may have a bigger cost. e.g. Cat classifier for kids, may show FP on adult images. The metric should have a higher cost on certain FP.</li>
</ul>
</body>
</html>
