<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Chinmay Hundekari" />
  <meta name="dcterms.date" content="2020-05-07" />
  <title>Normalizing activations in a network</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Normalizing activations in a network</h1>
<p class="author">Chinmay Hundekari</p>
<p class="date">May 07, 2020</p>
</header>
<nav id="TOC" role="doc-toc">
<h2 id="toc-title">Table of Contents</h2>
<ul>
<li><a href="#to-execute">To execute:</a></li>
<li><a href="#normalizing-activations-in-a-network">Normalizing activations in a network</a>
<ul>
<li><a href="#batch-normalization">Batch Normalization</a></li>
<li><a href="#fitting-batch-norm-into-a-neural-network">Fitting Batch Norm into a neural network</a>
<ul>
<li><a href="#feedforward-propagation">Feedforward Propagation</a></li>
<li><a href="#backpropagation">BackPropagation</a></li>
</ul></li>
</ul></li>
</ul>
</nav>
<h1 id="to-execute">To execute:</h1>
<p>pandoc Normalization.md -f markdown+tex_math_dollars -s -o Normalization.html â€“mathjax</p>
<h1 id="normalizing-activations-in-a-network">Normalizing activations in a network</h1>
<h2 id="batch-normalization">Batch Normalization</h2>
<ul>
<li><p>Easily train deep networks. Normalizing inputs helps to speed up learning. So, can we normalize z() or a() to speed up that layer.</p></li>
<li><p>Normalize each layer of z separately <span class="math display">\[ \mu = \frac{1}{m} \sum\limits_{i} z^i  \]</span> <span class="math display">\[ \sigma^{2} = \frac{1}{m} \sum\limits_{i} (z^i - \mu)^{2} \]</span> <span class="math display">\[ z^{i}_{norm} = \frac{z^{i} - \mu}{\sqrt{\sigma + \epsilon}}  \]</span> <span class="math display">\[ \widetilde{z}^{i} = \gamma z^{i}_{norm} + \beta  \]</span>, where <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\beta\)</span> are learnable parameters.</p></li>
<li><p>This would help in taking advantage of the non-linear region of sigmoid function, or any other activation function.</p></li>
</ul>
<h2 id="fitting-batch-norm-into-a-neural-network">Fitting Batch Norm into a neural network</h2>
<h3 id="feedforward-propagation">Feedforward Propagation</h3>
<ol type="1">
<li>Calculate <span class="math inline">\(z^{[l]}\)</span> for a layer.</li>
<li>Calculate batch norm <span class="math inline">\(\widetilde{z}^{[l]}\)</span> for <span class="math inline">\(z^{[l]}\)</span>, with <span class="math inline">\(\beta^{[l]}\)</span> and <span class="math inline">\(\gamma^{[l]}\)</span>.</li>
<li>Use <span class="math inline">\(\widetilde{z}^{[l]}\)</span> to calclulate <span class="math inline">\(a^{[l]}\)</span>.</li>
<li>Repeat for all layers.</li>
</ol>
<h3 id="backpropagation">BackPropagation</h3>
<ol type="1">
<li>Back propagate weight changes.</li>
<li>Back propagate <span class="math inline">\(\beta^{[l]}\)</span> changes with equation <span class="math inline">\(\beta^{[l]} = \beta^{[l]} - \alpha d\beta^{[l]}\)</span></li>
</ol>
</body>
</html>
