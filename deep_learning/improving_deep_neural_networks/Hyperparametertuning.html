<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Chinmay Hundekari" />
  <meta name="dcterms.date" content="2020-05-07" />
  <title>Hyperparameter Tuning</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Hyperparameter Tuning</h1>
<p class="author">Chinmay Hundekari</p>
<p class="date">May 07, 2020</p>
</header>
<nav id="TOC" role="doc-toc">
<h2 id="toc-title">Table of Contents</h2>
<ul>
<li><a href="#to-execute">To execute:</a></li>
<li><a href="#hyperparameter-tuning">Hyperparameter Tuning</a>
<ul>
<li><a href="#normalizing-activations-in-a-network">Normalizing activations in a network</a>
<ul>
<li><a href="#batch-normalization">Batch Normalization</a></li>
<li><a href="#fitting-batch-norm-into-a-neural-network">Fitting Batch Norm into a neural network</a></li>
<li><a href="#why-does-batch-norm-work">Why does Batch Norm work?</a></li>
<li><a href="#batch-norm-at-test-time">Batch norm at test time</a></li>
</ul></li>
<li><a href="#multi-class-classification">Multi-class Classification</a>
<ul>
<li><a href="#softmax-regression">Softmax Regression</a></li>
</ul></li>
<li><a href="#intoduction-to-deeplearning-frameworks---tensorflow">Intoduction to deeplearning frameworks - Tensorflow</a></li>
</ul></li>
</ul>
</nav>
<h1 id="to-execute">To execute:</h1>
<pre><code>pandoc HyperparameterTuning.md -f markdown+tex_math_dollars -s -o HyperparameterTuning.html --mathjax --toc -V toc-title:&quot;Table of Contents&quot;</code></pre>
<h1 id="hyperparameter-tuning">Hyperparameter Tuning</h1>
<h2 id="normalizing-activations-in-a-network">Normalizing activations in a network</h2>
<h3 id="batch-normalization">Batch Normalization</h3>
<ul>
<li><p>Easily train deep networks. Normalizing inputs helps to speed up learning. So, can we normalize z() or a() to speed up that layer.</p></li>
<li><p>Normalize each layer of z separately <span class="math display">\[ \mu = \frac{1}{m} \sum\limits_{i} z^i  \]</span> <span class="math display">\[ \sigma^{2} = \frac{1}{m} \sum\limits_{i} (z^i - \mu)^{2} \]</span> <span class="math display">\[ z^{i}_{norm} = \frac{z^{i} - \mu}{\sqrt{\sigma + \epsilon}}  \]</span> <span class="math display">\[ \widetilde{z}^{i} = \gamma z^{i}_{norm} + \beta  \]</span>, where <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\beta\)</span> are learnable parameters.</p></li>
<li><p>This would help in taking advantage of the non-linear region of sigmoid function, or any other activation function.</p></li>
</ul>
<h3 id="fitting-batch-norm-into-a-neural-network">Fitting Batch Norm into a neural network</h3>
<h4 id="feedforward-propagation">Feedforward Propagation</h4>
<ol type="1">
<li>Calculate <span class="math inline">\(z^{[l]}\)</span> for a layer.</li>
<li>Calculate batch norm <span class="math inline">\(\widetilde{z}^{[l]}\)</span> for <span class="math inline">\(z^{[l]}\)</span>, with <span class="math inline">\(\beta^{[l]}\)</span> and <span class="math inline">\(\gamma^{[l]}\)</span>.</li>
<li>Use <span class="math inline">\(\widetilde{z}^{[l]}\)</span> to calclulate <span class="math inline">\(a^{[l]}\)</span>.</li>
<li>Repeat for all layers.</li>
</ol>
<h4 id="backpropagation">BackPropagation</h4>
<ol type="1">
<li>Back propagate <span class="math inline">\(w^{[l]}\)</span> changes with equation <span class="math inline">\(w^{[l]} = w^{[l]} - \alpha \mathrm{d}w^{[l]}\)</span>.</li>
<li>Back propagate <span class="math inline">\(\beta^{[l]}\)</span> changes with equation <span class="math inline">\(\beta^{[l]} = \beta^{[l]} - \alpha \mathrm{d}\beta^{[l]}\)</span>.</li>
<li>Back propagate <span class="math inline">\(\gamma^{[l]}\)</span> changes with equation <span class="math inline">\(\gamma^{[l]} = \gamma^{[l]} - \alpha \mathrm{d}\gamma^{[l]}\)</span>.</li>
</ol>
<ul>
<li>Equation <span class="math inline">\(z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}\)</span> changes to <span class="math inline">\(z^{[l]} = W^{[l]}a^{[l-1]}\)</span>. This is because <span class="math inline">\(z\)</span> is normalized, and the averaging leads to loss of <span class="math inline">\(b\)</span>. This role is now taken by <span class="math inline">\(\beta\)</span>.</li>
</ul>
<h3 id="why-does-batch-norm-work">Why does Batch Norm work?</h3>
<ul>
<li>Works because we normalize weights.</li>
<li>Classifiers may not work well if data is shifted. e.g.Â Trained on black cats, and test on colored cats. This is called as covariance shift. Batch normalization reduces this, as each layer is independent, and common features may still persist in the data.</li>
<li>Effect of earlier layers on later layers is reduced due to normalizing weights.<br />
</li>
<li>Batch norm on mini batches adds noise after every batch to <span class="math inline">\(z^{[l]}\)</span>, forcing the layers to not rely on individual units. This adds slightly to regularization, just like dropout. This is not the main intent of batch norm. Smaller batches increase regularization effect.</li>
</ul>
<h3 id="batch-norm-at-test-time">Batch norm at test time</h3>
<ul>
<li>During test time, when we are running single test cases, it wont be possible to get meaningful values of <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\beta\)</span>.</li>
<li>This can be resolved by calculating a running exponential moving average of <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\beta\)</span> after every mini-batch training, and using the end result in test time.</li>
</ul>
<h2 id="multi-class-classification">Multi-class Classification</h2>
<h3 id="softmax-regression">Softmax Regression</h3>
<ul>
<li>The last layer has size equal to the number of classification categories (c).<br />
</li>
<li>For last layer, we calculate as: <span class="math display">\[ z^{[L]} = w^{[L]}a^{[L-1]} + b^{[L]} \]</span> <span class="math display">\[ t = e^{z{[L]}} \]</span> <span class="math display">\[ a^{[L]} = \frac{t_{i}}{\sum\limits_{i}t_{i}} \]</span></li>
<li>The final layer has normalized values which can be compared with each other and should sum up to 1.</li>
<li>Hardmax gets a single max value. Softmax is generalized logistic regression(c=2). ### Training a softmax classifier #### Loss function For class selected by setting x = 1: <span class="math display">\[ y^{i} = \begin{bmatrix}\\0\\...\\x \\... \\c \end{bmatrix} \]</span> <span class="math display">\[ \unicode{163} (\hat{y}, y) = \sum\limits_{j}{y_{j} log(\hat{y_{j}})} \]</span> gets reduced to <span class="math display">\[ \unicode{163} (\hat{y}, y) = y_{x} log(\hat{y_{x}}) \]</span> <span class="math display">\[ \unicode{163} (\hat{y}, y) = log(\hat{y_{x}}) \]</span> #### Backpropogation <span class="math display">\[ \mathrm{d}z^{[L]} = \hat{y} - y \]</span></li>
</ul>
<h2 id="intoduction-to-deeplearning-frameworks---tensorflow">Intoduction to deeplearning frameworks - Tensorflow</h2>
<ul>
<li>Always prefer opensource networks, where ownership is also open and not an organization. ### Tensorflow</li>
</ul>
<ol type="1">
<li>Create a computational graph describing the network.</li>
<li>Create placeholders for data</li>
</ol>
<pre><code>      X = tf.constant(np.random.randn(3,1), name = &quot;X&quot;)
      y = tf.constant(39, name=&#39;y&#39;)
      x = tf.placeholder(tf.float32, name = &quot;x&quot;)</code></pre>
<ol start="2" type="1">
<li>Run the graph with input data.</li>
</ol>
<ul>
<li>Basic structure</li>
</ul>
<pre><code>    y_hat = tf.constant(36, name=&#39;y_hat&#39;)            # Define y_hat constant. Set to 36.
    y = tf.constant(39, name=&#39;y&#39;)                    # Define y. Set to 39

    loss = tf.Variable((y - y_hat)**2, name=&#39;loss&#39;)  # Create a variable for the loss

    init = tf.global_variables_initializer()         # When init is run later (session.run(init)),
                                                     # the loss variable will be initialized and ready to be computed
    with tf.Session() as session:                    # Create a session and print the output
        session.run(init)                            # Initializes the variables
        print(session.run(loss))                     # Prints the loss</code></pre>
</body>
</html>
